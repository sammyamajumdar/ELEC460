{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extreme Learning Machine \n",
    "## Concept \n",
    "\n",
    "ELM is an machine learning algorithm developed by Guang-Bin Huang in 2006 that eliminates gradient based learning. Instead it uses Moore-Penrose Generalised Matrix pseudo inverse to calculate the output. Essentially it is a single layer feed forward neural network that initialises random weights and biases between the features the first hidden layer, making the output of the first hidden layer constant. The weights between the hidden layer and the output layer are then calculated using pseudo inverse of the output of the first hidden layer (Beta). This algorithm provides much shorter training time in comparison to generic gradient based learning techniques. \n",
    "\n",
    "Mathematically, a matrix B* will be a pseudo inverse of matrix B if it satisfies the following four conditions:\n",
    "1) BB* B = B\n",
    "2) B* BB* = B*\n",
    "3) (BB* )^H = BB*\n",
    "4) (B* B )^H = BB*\n",
    "where B^H is the conjugate transpose of matrix B. \n",
    "\n",
    "The shortest length least squares solution of the linear system Ax = c will be x = A* c where A* is the pseudo inverse of A. \n",
    "\n",
    "The implementation of the ELM algorithm is as follows:\n",
    "1) Initialise random weights and biases\n",
    "2) Calculate H where H = activation(X*W + b); In this implementation the sigmoid activation function is used.\n",
    "3) Derive Beta = H * y where y is the vector of true values and H** is the pseudo inverse of H. \n",
    "\n",
    "The pseudo inverse is calculated using the numpy linear algebra pinv command. So the final expression becomes H*Beta = y.\n",
    "\n",
    "Source: Guang-Bin Huang, Qin-Yu Zhu, Chee-Kheong Siew, Extreme learning machine: Theory and applications, Neurocomputing, Volume 70, Issues 1â€“3, 2006, Pages 489-501, ISSN 0925-2312.\n",
    "https://doi.org/10.1016/j.neucom.2005.12.126.\n",
    "(https://www.sciencedirect.com/science/article/pii/S0925231206000385)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sections\n",
    "\n",
    "The first section defines three functions: The sigmoid activation function since this is a regression problem, The ELM algorithm itself and the preds function that outputs predicted and true values from the test set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x)) \n",
    "def ELM(weights_shape, bias_shape, train_shape, train_X, train_y):\n",
    "    np.random.seed(42)\n",
    "    weights = np.random.rand(weights_shape[0], weights_shape[1])\n",
    "    bias = np.random.rand(bias_shape[0],bias_shape[1])\n",
    "    \n",
    "    \n",
    "    H = sigmoid(np.dot(train_X,weights) + bias)\n",
    "    H_pinv = np.linalg.pinv(H)\n",
    "    Beta = np.dot(H_pinv,train_y)\n",
    "    \n",
    "    print(\"Beta shape: \", Beta.shape)\n",
    "    \n",
    "    return Beta, weights, bias\n",
    "def preds(Beta, test_X, test_y, weights, bias):\n",
    "    H_pred = sigmoid(np.dot(test_X,weights)+bias)\n",
    "    y_pred = np.dot(H_pred, Beta)\n",
    "    \n",
    "    print(\"predictions: \", y_pred[:8])\n",
    "    print(\"true values: \", test_y[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section reads the dataset as a Pandas dataframe that has been regularised with the following formula: \n",
    "(x - x_mean) / x_range. This allows the NN model that deals with data in a similar range which improves the generalisation power. Upon reading the data, the target values (in this case the total CH4 conversion percentage) are split into a label dataset (y) and the rest is the features dataset (X). They are converted into a numpy as array and split into a training and testing dataset in the ratio 9:1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0   Co  Ba_promoter  Ca_promoter  Cu_promoter   Ce  Mn   La  \\\n",
      "0           4  0.0          0.0          0.0            0  0.0   0  0.0   \n",
      "1           5  0.0          0.0          0.0            0  0.0   0  0.0   \n",
      "2           6  0.0          0.0          0.0            0  0.0   0  0.0   \n",
      "3           7  0.0          0.0          0.0            0  0.0   0  0.0   \n",
      "4           8  0.0          0.0          0.0            0  0.0   0  0.0   \n",
      "\n",
      "         Ni   Pt  ...  ZSM-5  H-ZSM-5  react Temperature (C)  reac Ar %vol  \\\n",
      "0  0.100000  0.0  ...      0        0               0.636364           0.0   \n",
      "1  0.100000  0.0  ...      0        0               0.727273           0.0   \n",
      "2  0.100000  0.0  ...      0        0               0.818182           0.0   \n",
      "3  0.166667  0.0  ...      0        0               0.454545           0.0   \n",
      "4  0.166667  0.0  ...      0        0               0.545455           0.0   \n",
      "\n",
      "   reac N2 %vol  reac He %vol  GHSV (mgmin/mL)  Time on stream (min)  \\\n",
      "0           0.0             0         0.043708              0.007692   \n",
      "1           0.0             0         0.043708              0.007692   \n",
      "2           0.0             0         0.043708              0.007692   \n",
      "3           0.0             0         0.043708              0.007692   \n",
      "4           0.0             0         0.043708              0.007692   \n",
      "\n",
      "   feed_ratio   target  \n",
      "0         1.0  53.5601  \n",
      "1         1.0  75.8730  \n",
      "2         1.0  89.4785  \n",
      "3         1.0  38.5034  \n",
      "4         1.0  58.2766  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(\"regularised_data.xlsx\")\n",
    "print(data.head())\n",
    "y = data[\"target\"]\n",
    "X = data.drop(columns=[\"target\"])\n",
    "X = X.values\n",
    "y = y.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been reported in the literature that taking the number of nodes equal to the number of data points in the dataset makes it a powerful model with impressive results in some of the most common datasets. Hence I have taken the hidden node number to be 4967, equal to the number of data points in out training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_NODES = 4967\n",
    "weights_shape = (X_train.shape[1], HIDDEN_NODES)\n",
    "bias_shape = (1, HIDDEN_NODES)\n",
    "train_shape = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta shape:  (4967,)\n"
     ]
    }
   ],
   "source": [
    "B, weights, bias = ELM(weights_shape, bias_shape, train_shape, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the newly calculated Beta, some predictions are made on dataset and both are printed together to have a fair idea of the model's performance. So far, it isn't that impressive. From the looks of it, it's evident that the model is seriously underfitting the dataset. \n",
    "\n",
    "TO DO: Needs more work on the model complexity, further data processing, implementation of performance metric preferably mean squared error and/or mean absolute error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  [115.07421875  36.9296875   67.77734375  38.546875    49.55078125\n",
      "  62.0078125   59.7890625   38.1875    ]\n",
      "true values:  [63.7    18.9097 89.0187 37.1753 26.0982 56.7    97.7427 43.7551]\n"
     ]
    }
   ],
   "source": [
    "preds(B, X_test, y_test, weights, bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
